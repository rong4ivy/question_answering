{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guyez/NLP/blob/main/DistilBertQA_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lLFOMAf5qfA"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLUZHZKkM6S7",
        "outputId": "d42b196e-b3f1-47d5-adc8-743eff3bdfdf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q56PwL1LiG3N",
        "outputId": "cdfd674f-0610-4414-c0cb-3fbeab0f74dc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import collections\n",
        "from pathlib import Path\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "gENJjhhcs768"
      },
      "outputs": [],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ib8_3-Scxvk8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cellView": "form",
        "id": "gLClOaPt0luI"
      },
      "outputs": [],
      "source": [
        "# Folder on your Google Drive where all the checkpoints will be saved and where the dataset files are stored and loaded from\n",
        "FOLDER_NAME = \"NLPproject\" # @param {type: \"string\"}\n",
        "JSON_TEST_FILE = \"test_set.json\" # @param {type: \"string\"}\n",
        "data_path = \"drive/My Drive/\" + FOLDER_NAME +\"/\"  # Full path to Drive folder\n",
        "file_path = data_path + JSON_TEST_FILE\n",
        "checkpoint_path = data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dIGqe1a6rceN"
      },
      "outputs": [],
      "source": [
        "class LoadData():\n",
        "    def __init__(self,\n",
        "                 path_to_json_file: str,\n",
        "                 checkpoint_path: str,\n",
        "                 test_file: str = 'test.json') -> None:\n",
        "        \"\"\"Load the data by flattening the json file and saving it.\n",
        "\n",
        "          Args:\n",
        "              path_to_json_file: path to the json file.\n",
        "              checkpoint_path: path where to save the json file.\n",
        "              test_file: name of the test json file that will be created.\n",
        "        \"\"\"\n",
        "\n",
        "        self.path_to_json_file = path_to_json_file\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        self.test_file = test_file\n",
        "\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.path_to_json_file, 'r') as f:\n",
        "            test_data = json.load(f)\n",
        "        print(f'Flattening SQUAD {test_data[\"version\"]}')\n",
        "        test_data_flat, errors = self.load_squad_data(test_data)\n",
        "        print(f'\\nErroneous Datapoints: {errors}')\n",
        "\n",
        "        with open(Path(self.checkpoint_path) / Path(self.test_file), 'w') as file:\n",
        "            test_data = {'data':test_data_flat}\n",
        "            file.write(json.dumps(test_data))\n",
        "            file.close()\n",
        "\n",
        "\n",
        "    def load_squad_data(self, data):\n",
        "\n",
        "        errors = 0\n",
        "        flattened_data_test = []\n",
        "\n",
        "        for i, article in enumerate(data[\"data\"]):\n",
        "            title = article.get(\"title\", \"\").strip()\n",
        "            for paragraph in article[\"paragraphs\"]:\n",
        "                context = paragraph[\"context\"].strip()\n",
        "                for qa in paragraph[\"qas\"]:\n",
        "                    question = qa[\"question\"].strip()\n",
        "                    id_ = qa[\"id\"]\n",
        "\n",
        "                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
        "                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n",
        "\n",
        "                    # Features currently used are \"context\", \"question\", and \"answers\".\n",
        "                    # Others are extracted here for the ease of future expansions.\n",
        "                    flattened_data_test.append({\"title\": title,\n",
        "                                                \"context\": context,\n",
        "                                                \"question\": question,\n",
        "                                                \"id\": id_,\n",
        "                                                \"answers\": {\n",
        "                                                    \"answer_start\": answer_starts,\n",
        "                                                    \"text\": answers}\n",
        "                                                })\n",
        "\n",
        "        return flattened_data_test, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6DQzGjkiQaC",
        "outputId": "125697a6-143c-4670-9e13-1a3c767d9c5f"
      },
      "outputs": [],
      "source": [
        "_ = LoadData(file_path, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "f4db17ab397449bc95e5bd85e2b84795",
            "373100b427e54071a8fca63e090d1d4e",
            "f886282952da4ccd859fbe80dc66582b",
            "1802382d527d41cf801af8aa240625da",
            "5684f8e1baf945338e1d14c1067b63ed",
            "8931d5ce8a4040cb9fc2e3308e523475",
            "7408dd1438c84da0bbd235769cd3b421",
            "418c6caceafb43dc883ca0f173eece4e"
          ]
        },
        "id": "Ji9Vo1imO5Vq",
        "outputId": "c0006d65-d30d-4f2e-cfd8-06f34cbe6f1e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "test_data = load_dataset('json', data_files=data_path+\"test.json\", field='data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "b6twxoFaRNHK",
        "outputId": "60aa6c0c-37ce-4e13-fadb-1c376009311d"
      },
      "outputs": [],
      "source": [
        "def get_text(answer: list) -> str:\n",
        "    \"\"\"Extract only the text from the answers.text column \n",
        "\n",
        "    Args:\n",
        "        answer: the answer.\n",
        "    \"\"\"\n",
        "    return answer[0]\n",
        "\n",
        "def get_json_data(json_path: str) -> dict:\n",
        "    \"\"\"Get the json data in form of a dictionary\n",
        "\n",
        "    Args:\n",
        "        json_path: path to the json file.\n",
        "    \"\"\"\n",
        "    # Opening JSON file \n",
        "    f = open(json_path) \n",
        "    # returns JSON object as a dictionary \n",
        "    json_data = json.load(f) \n",
        "    # Closing file \n",
        "    f.close() \n",
        "    return json_data\n",
        "\n",
        "test_dataframe = pd.json_normalize(get_json_data(data_path+\"test.json\"), record_path='data')\n",
        "test_dataframe[\"answers.text\"] = test_dataframe[\"answers.text\"].apply(get_text)\n",
        "\n",
        "test_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "uvVkdnU17gEt",
        "outputId": "7c98883f-2be7-4d8b-d9a3-b923d8ea2cd2"
      },
      "outputs": [],
      "source": [
        "figsize = (10,6)\n",
        "test_dataframe['context'].apply(len).plot.hist(title=\"Contex length histogram\", bins=20, figsize=figsize, grid=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Jab-fQ1X8dfY",
        "outputId": "d158cf9d-7acb-4d66-d790-5d688301d617"
      },
      "outputs": [],
      "source": [
        "test_dataframe['question'].apply(len).plot.hist(title=\"Question length histogram\", bins=20, figsize=figsize, grid=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "lY9PhtcW8pmt",
        "outputId": "a675fddf-a856-401b-f269-21370799ef64"
      },
      "outputs": [],
      "source": [
        "test_dataframe['answers.text'].apply(len).plot.hist(title=\"Answer length histogram\", bins=20, figsize=figsize, grid=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGYzWRfv2SuH",
        "outputId": "47f90ce9-b34e-4019-c3dc-2563a9761d6c"
      },
      "outputs": [],
      "source": [
        "#@title Show Samples { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "def print_squad_sample(df: pd.core.frame.DataFrame, line_length: int=20, separator_length: int=120) -> None:\n",
        "  sample = df.iloc[random.randint(0,df.shape[0])]\n",
        "  title = sample.title.replace('_', ' ')\n",
        "  print('TITLE: ')\n",
        "  print(title)\n",
        "  print('='*separator_length)\n",
        "  context = sample.context.split()\n",
        "  print('CONTEXT: ')\n",
        "  lines = [' '.join(context[idx:idx+line_length]) for idx in range(0, len(context), line_length)]\n",
        "  for l in lines:\n",
        "      print(l)\n",
        "  print('='*separator_length)\n",
        "  questions = df[df.context.values==sample.context]\n",
        "  max_len = len(max(questions.question, key=len)) + 5\n",
        "  print(\"{: <{max_len}} {: <{max_len}}\".format('QUESTION:','ANSWER:', max_len=max_len))\n",
        "  for idx, row in questions.iterrows():\n",
        "    question = row.question\n",
        "    answer = row['answers.text']\n",
        "    print(\"{: <{max_len}} {: <{max_len}}\".format(question,answer, max_len=max_len))\n",
        "    \n",
        "print_squad_sample(test_dataframe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inog-64XTyd0"
      },
      "source": [
        "#Preprocessing the data\n",
        "\n",
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell.\n",
        "\n",
        "As model_checkpoint we use the best performing version of our DistilBertForQuestionAnswering which we have uploaded on the HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "oOEF4DScUDPi"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"nlpunibo/distilbert_config3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AhmVAsCrXKD9"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "pad_on_right = True # Our model expects padding on the right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqOg2iAEdrEB"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2KRX94OdzPD"
      },
      "source": [
        "We can download the pretrained model. We use our modified version of the `DistilBertForQuestionAnswering` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rV0pZMjOPly2"
      },
      "outputs": [],
      "source": [
        "# DistilBert\n",
        "\n",
        "import math\n",
        "\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "from transformers import DistilBertPreTrainedModel\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n",
        "    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +\n",
        "    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in\n",
        "    torch.nn.functional Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "\n",
        "class DistilBertForQuestionAnswering(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "\n",
        "        self.qa_outputs_0 = nn.Linear(config.dim, 512)\n",
        "        self.qa_outputs_1 = nn.Linear(512, 32)\n",
        "        self.qa_outputs = nn.Linear(32, config.num_labels)\n",
        "        \n",
        "        assert config.num_labels == 2\n",
        "        self.dropout = nn.Dropout(config.qa_dropout)\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(normalized_shape = [384,2])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "\n",
        "        distilbert_output = self.distilbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        " \n",
        "        hidden_states = distilbert_output[0]  # (bs, max_query_len, dim)\n",
        "        hidden_states = self.dropout(hidden_states)  # (bs, max_query_len, dim)\n",
        "\n",
        "        logits = gelu_new(self.qa_outputs_0(hidden_states))  # (bs, max_query_len, 2)\n",
        "        logits = gelu_new(self.qa_outputs_1(logits))\n",
        "        #logits = self.LayerNorm_0(logits)\n",
        "      \n",
        "        logits = self.qa_outputs(logits)\n",
        "        logits = self.LayerNorm(logits)\n",
        "       \n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)  # (bs, max_query_len)\n",
        "        end_logits = end_logits.squeeze(-1)  # (bs, max_query_len)\n",
        "\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "          output = (start_logits, end_logits) + distilbert_output[1:]\n",
        "          return ((total_loss,) + output) if total_loss is not None else output\n",
        " \n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=distilbert_output.hidden_states,\n",
        "            attentions=distilbert_output.attentions\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "SVzYPE4Pd2tY"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgtKN3wsfeBU"
      },
      "source": [
        "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call `n_best_size`. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one.\n",
        "\n",
        "We also eliminate very long answers from our considerations (with an hyper-parameter we can tune).\n",
        "\n",
        "Since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. We need to build a map from example index to its corresponding features indices.\n",
        "\n",
        "\n",
        " All combined together, this gives us this `post-processing` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2FlROrJIi9Th"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "def postprocess_qa_predictions(examples: datasets.arrow_dataset.Dataset,\n",
        "                               features: datasets.arrow_dataset.Dataset,\n",
        "                               raw_predictions: tuple,\n",
        "                               n_best_size: int = 20,\n",
        "                               max_answer_length: int = 50) -> collections.OrderedDict:\n",
        "    \"\"\"Function used to select the best answer from the raw predictions\n",
        "\n",
        "      Args:\n",
        "        examples: Squad samples\n",
        "        features: Squad features\n",
        "        raw_predictions: model predictions\n",
        "    \"\"\"\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        # Let's pick our final answer\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIIlk6R3eC40"
      },
      "source": [
        "The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "\n",
        "- the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "- the offset mapping that will give us a map from token indices to character positions in the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "LZoJcV9_hmqq"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples: collections.OrderedDict or dict) -> transformers.tokenization_utils_base.BatchEncoding:\n",
        "    \"\"\"To check a given span is inside the context (and not the question) and to get back the text inside.\n",
        "        To do this, we need to add two things to our validation features:\n",
        "        - the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "        - the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "        That's why we will re-process the validation set with the following function, slightly different from `prepare_train_features`\n",
        "\n",
        "      Args:\n",
        "        examples: Squad samples\n",
        "    \"\"\"\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "      # CLS index\n",
        "      input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "      cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "      # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "      sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "      # One example can give several spans, this is the index of the example containing this span of text.\n",
        "      sample_index = sample_mapping[i]\n",
        "      answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "      # Start/end character index of the answer in the text.\n",
        "      start_char = answers[\"answer_start\"][0]\n",
        "      end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "      # Start token index of the current span in the text.\n",
        "      token_start_index = 0\n",
        "      while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "          token_start_index += 1\n",
        "\n",
        "      # End token index of the current span in the text.\n",
        "      token_end_index = len(input_ids) - 1\n",
        "      while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "          token_end_index -= 1\n",
        "\n",
        "      # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "      if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "          tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "          tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "      else:\n",
        "          # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "          # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "          while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "              token_start_index += 1\n",
        "          tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "          while offsets[token_end_index][1] >= end_char:\n",
        "              token_end_index -= 1\n",
        "          tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "76888d142095450d8d3d647d8e656cf0",
            "7c753b376ada4796851c2fa12aa68737",
            "367b092658f446eba2bfda94a98c0bbf",
            "62f93f5e71014ccdbda345d3d08f41bb",
            "8dd4a84d9e11433bbeb12b63fd2b8aa5",
            "73ea8604d9724719ae9818b4cde32a36",
            "b8e70c07d817459a8df6ba987264b4e2",
            "718cf3570e3c4696a0299bf6bc12c870"
          ]
        },
        "id": "WhidQb3ljW6n",
        "outputId": "ae531a11-f9c6-4759-ee2e-24744ab736ce"
      },
      "outputs": [],
      "source": [
        "test_features = test_data['train'].map(prepare_validation_features, batched=True, remove_columns=test_data['train'].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYjOLpYMtmXe"
      },
      "source": [
        "We instantiate a `Trainer`, that will be used to get the predictions.\n",
        "Note: This is not necessary, but using the Trainer instead of directly the model to get the predictions simplify this operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "K82JqsZ3thEC"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    label_names=[\"start_positions\", \"end_positions\"]\n",
        ")\n",
        "trainer = Trainer(model, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "I0G-l4GLufC9",
        "outputId": "bb9b5b65-99a4-4ad6-bae8-ad931b83361c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1348' max='1348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1348/1348 04:37]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get final predictions\n",
        "with torch.no_grad():\n",
        "    pred = trainer.predict(test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76GsMSs7upgs",
        "outputId": "29a6d657-6c04-4ca2-e56a-c8aa9c2e161c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:24<00:00, 428.27it/s]\n"
          ]
        }
      ],
      "source": [
        "# The Trainer hides the columns that are not used by the model (here example_id and offset_mapping which we will need for our post-processing), so we set them back\n",
        "test_features.set_format(type=test_features.format[\"type\"],\n",
        "                          columns=list(test_features.features.keys()))\n",
        "\n",
        "# To get the final predictions we can apply our post-processing function to our raw predictions\n",
        "final_predictions = dict(postprocess_qa_predictions(test_data['train'], test_features, pred.predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "AHiYByi-uv54"
      },
      "outputs": [],
      "source": [
        "# Create a new file and save the predictions\n",
        "with open(data_path + \"predictions.json\", 'w') as file:\n",
        "    file.write(json.dumps(final_predictions))\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGJyJxn1Ujhc"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "sxsWHv8I5HgR"
      },
      "outputs": [],
      "source": [
        "formatted_predictions = {k : v for k, v in final_predictions.items()}\n",
        "metric = datasets.load_metric(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHgwYht3Ujhg",
        "outputId": "04ae8f99-a9eb-42f1-89fc-f0944bdc2d43"
      },
      "outputs": [],
      "source": [
        "#@title Show Results { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "def print_squad_sample(df: pd.core.frame.DataFrame, formatted_predictions: dict, line_length: int=20, separator_length: int=120) -> None:\n",
        "  sample = df.iloc[random.randint(0,df.shape[0])]\n",
        "  title = sample.title.replace('_', ' ')\n",
        "  print('TITLE: ')\n",
        "  print(title)\n",
        "  print('='*separator_length)\n",
        "  context = sample.context.split()\n",
        "  print('CONTEXT: ')\n",
        "  lines = [' '.join(context[idx:idx+line_length]) for idx in range(0, len(context), line_length)]\n",
        "  for l in lines:\n",
        "      print(l)\n",
        "  print('='*separator_length)\n",
        "  questions = df[df.context.values==sample.context]\n",
        "\n",
        "  max_len_question = len(max(questions.question, key=len)) + 5\n",
        "  max_len_answer = len(max(questions[\"answers.text\"], key=len)) + 5 \n",
        "\n",
        "  print(\"{: <{max_len_question}} {: <{max_len_answer}} {: <{max_len_answer}} {}\".format('QUESTION:','ANSWER:', \"PREDICTION:\", \"EM?\", max_len_question=max_len_question, max_len_answer= max_len_answer))\n",
        "  for idx, row in questions.iterrows():\n",
        "    question = row.question\n",
        "    answer = row[\"answers.text\"]\n",
        "    predicted = formatted_predictions[row.id]\n",
        "    correct = \"âœ”\" if predicted == answer else \"âœ˜\"\n",
        "    print(\"{: <{max_len_question}} {: <{max_len_answer}} {: <{max_len_answer}} {}\".format(question,answer,predicted,correct, max_len_question=max_len_question, max_len_answer = max_len_answer))\n",
        "\n",
        "df = pd.json_normalize(get_json_data(data_path+\"test.json\"), record_path='data')\n",
        "df[\"answers.text\"] = df[\"answers.text\"].apply(get_text)\n",
        "print_squad_sample(df, formatted_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "6qsmnexZK6kJ"
      },
      "outputs": [],
      "source": [
        "test_dataframe['prediction_text'] = formatted_predictions.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "3fnXhb1Brik1",
        "outputId": "1dba71eb-08f3-44ef-9ea0-5f94c3447422"
      },
      "outputs": [],
      "source": [
        "test_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "6k5J_k5TO8eJ"
      },
      "outputs": [],
      "source": [
        "# Function to get the questions' head\n",
        "def get_5w(question: str) -> str:\n",
        "    \"\"\"Extract only the \"head\" froma a question\n",
        "\n",
        "    Args:\n",
        "        question: the question.\n",
        "    \"\"\"\n",
        "    return question.split()[0].strip().lower()\n",
        "test_dataframe['question'] = test_dataframe['question'].apply(get_5w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8zEnUQKUQvu",
        "outputId": "885863a1-0ccd-438a-aa0e-3415a4fce62f"
      },
      "outputs": [],
      "source": [
        "total = test_dataframe.shape[0]\n",
        "top_qheads = test_dataframe['question'].value_counts().sort_values(ascending=False)[0:25]\n",
        "qheads = list(top_qheads.index)\n",
        "count = top_qheads.to_list()\n",
        "\n",
        "print(\"Label\\t\\tCount\\t\\tPercentage\\n\")\n",
        "for i, j in enumerate(top_qheads):\n",
        "  print(\"{: <15} {: <15} {:.1f}%\".format(qheads[i],j,j/total*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0xhPDgvXZwB",
        "outputId": "ddba311d-3c3a-4131-b858-f9b1a0a8a8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'exact_match': 74.07407407407408, 'f1': 80.87301587301587}\n"
          ]
        }
      ],
      "source": [
        "#@title Show Statistics { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "question_head = 'for' #@param ['what', 'how', 'who', 'when', 'which', 'in', 'where', 'why', 'the', 'during', 'along', 'on', 'at', 'to', 'from', 'for', 'a', 'by', 'whose', 'is', 'according', 'after', 'if', 'name', 'as']\n",
        "temp = test_dataframe\n",
        "temp['answers'] = test_data[\"train\"]['answers']\n",
        "temp = test_dataframe[test_dataframe['question'].isin([question_head])]\n",
        "final_predictions = temp[['id','prediction_text']].to_dict('records')\n",
        "references = temp[['answers','id']].to_dict('records')\n",
        "metrics = metric.compute(predictions=final_predictions, references=references)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "B9P3RPAQbUfe"
      },
      "outputs": [],
      "source": [
        "f1_list = []\n",
        "em_list = []\n",
        "\n",
        "for q in qheads:\n",
        "  temp = test_dataframe\n",
        "  temp['answers'] = test_data[\"train\"]['answers']\n",
        "  temp = test_dataframe[test_dataframe['question'].isin([q])]\n",
        "  final_predictions = temp[['id','prediction_text']].to_dict('records')\n",
        "  references = temp[['answers','id']].to_dict('records')\n",
        "  metrics = metric.compute(predictions=final_predictions, references=references)\n",
        "  em_list.append(metrics['exact_match'])\n",
        "  f1_list.append(metrics['f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KCvPCdcyuG3",
        "outputId": "104c34ef-ba34-4538-c006-26bac5bbbd81"
      },
      "outputs": [],
      "source": [
        "qheads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "gFsPAP-XcgSP",
        "outputId": "b53164a7-22ad-492c-e1ac-8b1fe1ef0268"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1 = count\n",
        "y1 = em_list\n",
        "\n",
        "plt.figure(figsize=(9,5))\n",
        "\n",
        "plt.scatter(x1, y1, label = \"line 1\")\n",
        "plt.xlabel('Occurrences')\n",
        "plt.ylabel('Exact match')\n",
        "\n",
        "for i, q in enumerate(qheads):\n",
        "    plt.annotate(q, (count[i], em_list[i]),  (count[i], em_list[i] + 1))\n",
        "\n",
        "# Display a figure.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "bEEelg3leqQz",
        "outputId": "027c47f4-ad33-4a3d-963d-b8e0775b367e"
      },
      "outputs": [],
      "source": [
        "x1 = count\n",
        "y1 = f1_list\n",
        "\n",
        "plt.figure(figsize=(9,5))\n",
        "\n",
        "plt.scatter(x1, y1, label = \"line 1\")\n",
        "plt.xlabel('Occurrences')\n",
        "plt.ylabel('F1 Score')\n",
        "\n",
        "for i, q in enumerate(qheads):\n",
        "    plt.annotate(q, (count[i], f1_list[i]),  (count[i], f1_list[i] + 1))\n",
        "\n",
        "# Display a figure.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "vDmWsz7rARWv",
        "outputId": "2ff2dce0-9a46-4bda-e559-481e83427ba6"
      },
      "outputs": [],
      "source": [
        "def get_char_diff(row: pd.core.series.Series) -> int:\n",
        "    \"\"\"Compute the difference, in terms of the number of different charecters\n",
        "       between the real answer and the predicted one\n",
        "    \"\"\"\n",
        "    return sum(1 for a, b in zip(row[\"answers.text\"], row.prediction_text) if a != b) + abs(len(row[\"answers.text\"]) - len(row.prediction_text))\n",
        "  \n",
        "test_dataframe['difference'] = test_dataframe.apply(get_char_diff, axis=1)\n",
        "test_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "T--sjiI0DvLb",
        "outputId": "e0ca5fc6-827a-482a-b42f-9f0f57b318ce"
      },
      "outputs": [],
      "source": [
        "figsize = (10,6)\n",
        "test_dataframe['difference'].plot.hist(title=\"Number of different characthers\", bins= 50, figsize=figsize, grid=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i36DRBNh3mTw",
        "outputId": "62f513d5-38f8-4ca0-e1cc-348326877ce6"
      },
      "outputs": [],
      "source": [
        "total = test_dataframe['difference'][test_dataframe['difference'] != 0].shape[0]\n",
        "sorted_series = test_dataframe['difference'][test_dataframe['difference'] != 0].value_counts().sort_values(ascending=False)[0:25]\n",
        "numbers = list(sorted_series.index)\n",
        "\n",
        "print(\"# Diff.Chars\\tCount\\t\\tPercentage\\n\")\n",
        "for i, j in enumerate(sorted_series):\n",
        "  print(\"{: <15} {: <15} {:.1f}%\".format(numbers[i],j,j/total*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "DoHxa6w1dHGG"
      },
      "outputs": [],
      "source": [
        "# This function instead of returning the best prediction, returns the best 5 predictions\n",
        "\n",
        "def postprocess_qa_5predictions(examples: datasets.arrow_dataset.Dataset,\n",
        "                               features: datasets.arrow_dataset.Dataset,\n",
        "                               raw_predictions: tuple,\n",
        "                               n_best_size: int = 20,\n",
        "                               max_answer_length: int = 50) -> collections.OrderedDict:\n",
        "    \"\"\"Function used to select the 5 top answers from the raw predictions\n",
        "\n",
        "      Args:\n",
        "        examples: Squad samples\n",
        "        features: Squad features\n",
        "        raw_predictions: model predictions\n",
        "    \"\"\"\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0:5]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = [{\"text\": \"\", \"score\": 0.0}]\n",
        "        \n",
        "        # Let's pick our final answer\n",
        "        predictions[example[\"id\"]] = [a_dict[\"text\"] for a_dict in best_answer] \n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTKvxljjdNMH",
        "outputId": "1a73b39d-ad2a-4755-d553-2bfcf8c54cb3"
      },
      "outputs": [],
      "source": [
        "# The Trainer hides the columns that are not used by the model (here example_id and offset_mapping which we will need for our post-processing), so we set them back\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\n",
        "\n",
        "# To get the final predictions we can apply our post-processing function to our raw predictions\n",
        "final_predictions = postprocess_qa_5predictions(test_data['train'], test_features, pred.predictions)\n",
        "\n",
        "formatted_5predictions = {k : v for k, v in final_predictions.items()}\n",
        "\n",
        "# Hide again the columns that are not used by the model\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=['attention_mask', 'end_positions', 'input_ids', 'start_positions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iInYaPT4fGnr",
        "outputId": "e5fc502d-ac67-4ba7-a936-174ccca93468"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for id in test_dataframe['id']:\n",
        "  if test_dataframe['answers.text'][test_dataframe.id == id].values[0] in formatted_5predictions[id]:\n",
        "    count += 1\n",
        "    \n",
        "final_predictions = test_dataframe[['id','prediction_text']].to_dict('records')\n",
        "references = test_dataframe[['answers','id']].to_dict('records')\n",
        "\n",
        "print(\"Top 1 exact match and F1 score: {}\".format(metric.compute(predictions=final_predictions, references=references)))\n",
        "print(\"Top 5 exact match: {}\".format(count/len(test_dataframe)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "DistilBertQA_eval.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1802382d527d41cf801af8aa240625da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_418c6caceafb43dc883ca0f173eece4e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7408dd1438c84da0bbd235769cd3b421",
            "value": " 1/? [00:00&lt;00:00,  2.04 tables/s]"
          }
        },
        "367b092658f446eba2bfda94a98c0bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73ea8604d9724719ae9818b4cde32a36",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dd4a84d9e11433bbeb12b63fd2b8aa5",
            "value": 11
          }
        },
        "373100b427e54071a8fca63e090d1d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418c6caceafb43dc883ca0f173eece4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5684f8e1baf945338e1d14c1067b63ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "62f93f5e71014ccdbda345d3d08f41bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_718cf3570e3c4696a0299bf6bc12c870",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8e70c07d817459a8df6ba987264b4e2",
            "value": " 11/11 [05:36&lt;00:00, 30.57s/ba]"
          }
        },
        "718cf3570e3c4696a0299bf6bc12c870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ea8604d9724719ae9818b4cde32a36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7408dd1438c84da0bbd235769cd3b421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76888d142095450d8d3d647d8e656cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_367b092658f446eba2bfda94a98c0bbf",
              "IPY_MODEL_62f93f5e71014ccdbda345d3d08f41bb"
            ],
            "layout": "IPY_MODEL_7c753b376ada4796851c2fa12aa68737"
          }
        },
        "7c753b376ada4796851c2fa12aa68737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8931d5ce8a4040cb9fc2e3308e523475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd4a84d9e11433bbeb12b63fd2b8aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b8e70c07d817459a8df6ba987264b4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4db17ab397449bc95e5bd85e2b84795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f886282952da4ccd859fbe80dc66582b",
              "IPY_MODEL_1802382d527d41cf801af8aa240625da"
            ],
            "layout": "IPY_MODEL_373100b427e54071a8fca63e090d1d4e"
          }
        },
        "f886282952da4ccd859fbe80dc66582b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8931d5ce8a4040cb9fc2e3308e523475",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5684f8e1baf945338e1d14c1067b63ed",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
