
# Question Answering Model Performance Comparison
## Overview
This project aims to compare the performance of different pre-trained models in the field of question answering. Question answering (QA) is a natural language processing (NLP) task where models are designed to understand and respond to questions posed in natural language.

The project evaluates the accuracy, speed, and resource requirements of various pre-trained models to determine their suitability for different QA tasks.

## Pre-trained Models for comparison:

BERT 
DistilBertModel
RoBERTa 
GPT-3

## Dataset
SQuAD, which stands for Stanford Question Answering Dataset, is a popular and widely used benchmark dataset in the field of natural language processing (NLP) and machine learning. It was created by researchers at Stanford University and is designed to evaluate the ability of NLP models to comprehend and answer questions in a context, demonstrating the models' reading comprehension and question-answering capabilities.

## Evaluation Metrics
The following metrics are used to evaluate and compare the performance of the pre-trained models:<br>
Accuracy: Measures the correctness of the answers provided by the models.<br>
Inference Speed: Measures the time taken for each model to generate answers to a set of questions.<br>
Resource Usage: Assesses the computational resources (e.g., GPU memory, CPU utilization) required by each model.
